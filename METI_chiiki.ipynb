{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "METI_chiiki.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aokit/Google_Cloud_Functions_Example1/blob/master/METI_chiiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Obn19IeCa3U4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1cb91a8a-b1a4-40d2-ae52-287fb83c4922"
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import pprint\n",
        "import requests\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "\n",
        "class Parser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        HTMLParser.__init__(self)\n",
        "        self.title = False\n",
        "        self.link = False\n",
        "        self.data = []\n",
        "\n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        attrs = dict(attrs)\n",
        "#       if tag == \"h2\" and \"class\" in attrs and attrs['class'] == \"esc-lead-article-title\":\n",
        "        if tag == \"h1\" :\n",
        "            self.data.append({})\n",
        "            self.title = True\n",
        "            self.link = True\n",
        "            '''            \n",
        "        if tag == \"a\" and self.link == True:\n",
        "            self.data[-1].update({\"link\": attrs[\"href\"]})\n",
        "            '''\n",
        "\n",
        "    def handle_data(self, data):\n",
        "        True\n",
        "'''\n",
        "        if self.title == True or self.link == True:\n",
        "            self.data[-1].update({\"title\": data})\n",
        "            self.title = False\n",
        "            self.link = False\n",
        "'''\n",
        "\n",
        "def main():\n",
        "    payload = {\"topic\" : \"t\"}\n",
        "    # r = requests.get('https://news.google.com/news/section', params=payload)\n",
        "    r = requests.get('http://www.meti.go.jp/policy/anpo/apply08.html', params=payload)\n",
        "    # pprint.pprint(r.text)\n",
        "    parser = Parser()\n",
        "    parser.feed(r.text)\n",
        "    parser.close()\n",
        "\n",
        "    pprint.pprint(parser.data)\n",
        "    for i in parser.data:\n",
        "#       print(\"Title: \" + i[\"title\"], \"\\nLink: \" + i[\"link\"] + \"\\n\")\n",
        "#       print(\"Title: \" + i[\"title\"], \"\\n\")\n",
        "#       print(\"Title: \" , \"\\n\")\n",
        "        print(type(i))\n",
        "        pprint.pprint(i)\n",
        "        for k in i:\n",
        "            print(k)\n",
        "#       s = i[\"title\"]\n",
        "#       s = i[\"h1\"]\n",
        "#       b = s.encode('cp932' , \"ignore\")\n",
        "#       s_after = b.decode('cp932')\n",
        "#       print(\": \" + s_after, \"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9GQzjrBPdbp_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "import pprint\n",
        "import requests\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "\n",
        "class Parser(HTMLParser):\n",
        "    def __init__(self):\n",
        "        HTMLParser.__init__(self)\n",
        "        self.title = False\n",
        "        self.link = False\n",
        "        self.data = []\n",
        "\n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        attrs = dict(attrs)\n",
        "        if tag == \"h2\" and \"class\" in attrs and attrs['class'] == \"esc-lead-article-title\":\n",
        "            self.data.append({})\n",
        "            self.title = True\n",
        "            self.link = True\n",
        "\n",
        "        if tag == \"a\" and self.link == True:\n",
        "            self.data[-1].update({\"link\": attrs[\"href\"]})\n",
        "\n",
        "    def handle_data(self, data):\n",
        "        if self.title == True or self.link == True:\n",
        "            self.data[-1].update({\"title\": data})\n",
        "            self.title = False\n",
        "            self.link = False\n",
        "\n",
        "\n",
        "def main():\n",
        "    payload = {\"topic\" : \"t\"}\n",
        "    # r = requests.get('https://news.google.com/news/section', params=payload)\n",
        "    # r = requests.get('https://news.google.com/?hl=ja&gl=JP&ceid=JP:ja', params=payload)\n",
        "    r = requests.get('https://news.google.com/')\n",
        "    pprint.pprint(r.text)\n",
        "    parser = Parser()\n",
        "    parser.feed(r.text)\n",
        "    parser.close()\n",
        "\n",
        "    for i in parser.data:\n",
        "        print(\"Title: \" + i[\"title\"], \"\\nLink: \" + i[\"link\"] + \"\\n\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gft_gzrlgkj2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import requests, bs4\n",
        "# res = requests.get('https://tonari-it.com')\n",
        "res = requests.get('http://www.meti.go.jp/policy/anpo/apply08.html')\n",
        "res.raise_for_status()\n",
        "#  soup = bs4.BeautifulSoup(res.text, \"html.parser\")\n",
        "soup = bs4.BeautifulSoup(res.content, \"html.parser\")\n",
        "rr=soup.title\n",
        "tt=pprint.pformat(rr)\n",
        "# ss=tt.encode('cp932' , \"ignore\")\n",
        "# uu=ss.decode('cp932')\n",
        "# print(soup.title)\n",
        "# print(uu)\n",
        "# print(soup.h1)\n",
        "# print(soup.td)\n",
        "# print(soup.strong)\n",
        "# print(soup.name)\n",
        "# print(soup.a)\n",
        "# print(soup.td)\n",
        "'''for a in soup.find_all('a'):\n",
        "      print(a.get('href')) \n",
        "'''\n",
        "for a in soup.find_all('a'):\n",
        "      print(a.get('name')) \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4LiurJbHsXwQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### -*- coding:utf-8 -*-\n",
        "# METI_chiiki.py\n",
        "\n",
        "# https://qiita.com/hujuu/items/b0339404b8b0460087f9\n",
        "# これを参考にさせていただきました。ありがとうございます。\n",
        "\n",
        "import pprint\n",
        "import csv\n",
        "# from urllib.request import urlopen\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#URLの指定\n",
        "# html = urlopen(\"https://www.oreilly.co.jp/ebook/\")\n",
        "'''\n",
        "html = urlopen(\"http://www.meti.go.jp/policy/anpo/apply08.html\")\n",
        "bsObj = BeautifulSoup(html, \"html.parser\")\n",
        "# urlopenだとうまく動かないので、requests.get で .content を得るようにしました。\n",
        "# これは\n",
        "# https://orangain.hatenablog.com/entry/encoding-in-requests-and-beautiful-soup\n",
        "# これを参考にさせていただきました。ありがとうございます。\n",
        "# chardet も入れておいたほうがいいのかもしれませんが、今は入れてません。\n",
        "'''\n",
        "\n",
        "res = requests.get('http://www.meti.go.jp/policy/anpo/apply08.html')\n",
        "res.raise_for_status()\n",
        "bsObj = bs4.BeautifulSoup(res.content , \"html.parser\")\n",
        "\n",
        "#テーブルを指定\n",
        "# table = bsObj.findAll(\"table\",{\"class\":\"tablesorter\"})[0]\n",
        "tables = bsObj.findAll(\"table\",{\"class\" : \"style71\"})\n",
        "\n",
        "csvFileName ='chiiki_work.csv'\n",
        "\n",
        "csvFile = open(csvFileName , 'wt', newline = '', encoding = 'utf-8')\n",
        "writer = csv.writer(csvFile)\n",
        "\n",
        "try:\n",
        "  for table in tables:\n",
        "    rows = table.findAll(\"tr\")\n",
        "    for row in rows:\n",
        "        csvRow = []\n",
        "        for cell in row.findAll(['td', 'th']):\n",
        "            # csvRow.append(cell.get_text().lstrip().replace('\"',''))\n",
        "            # もともとのページの作りが、文字列の前後がわりと雑に\n",
        "            # 空白などが入っていて、それもここで切りおとす。\n",
        "            csvRow.append(cell.get_text().lstrip().rstrip())\n",
        "        # print(csvRow[0])\n",
        "        listRow = csvRow[0].replace('、',',').split()\n",
        "        pprint.pprint(listRow)\n",
        "        # writer.writerow(csvRow)\n",
        "        # writer.writerow(pprint.pformat(listRow))\n",
        "        \n",
        "finally:\n",
        "    csvFile.close()\n",
        "\n",
        "with open(csvFileName , 'rt') as file:\n",
        "  s = file.read()\n",
        "  print(s)\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pW9YKyEYRXAw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "### -*- coding:utf-8 -*-\n",
        "# METI_chiiki.py\n",
        "\n",
        "# 対象とする Web ページから決める記述は以下の３行。以下は\n",
        "# 経産省の国と地域のページから地域の定義表を取り出すため\n",
        "# に必要となる情報\n",
        "Surl = \"http://www.meti.go.jp/policy/anpo/apply08.html\"\n",
        "Sdescription = \"table\"\n",
        "Ddescription = {\"class\" : \"style71\"}\n",
        "\n",
        "# https://qiita.com/hujuu/items/b0339404b8b0460087f9\n",
        "# これを参考にさせていただきました。ありがとうございます。\n",
        "\n",
        "import pprint\n",
        "import csv\n",
        "import requests\n",
        "import bs4\n",
        "import hashlib\n",
        "\n",
        "# URLを指定してコンテンツを取り出す\n",
        "#\n",
        "'''\n",
        "html = urlopen(\"http://www.meti.go.jp/policy/anpo/apply08.html\")\n",
        "bsObj = BeautifulSoup(html, \"html.parser\")\n",
        "# urlopenだとうまく動かないので、requests.get で .content を得るようにしました。\n",
        "# これは\n",
        "# https://orangain.hatenablog.com/entry/encoding-in-requests-and-beautiful-soup\n",
        "# これを参考にさせていただきました。ありがとうございます。\n",
        "# chardet も入れておいたほうがいいのかもしれませんが、今は入れてません。\n",
        "'''\n",
        "# 文法解析されたコンテンツを bsObj に入れておく\n",
        "# Surl = \"http://www.meti.go.jp/policy/anpo/apply08.html\"\n",
        "res = requests.get(Surl)\n",
        "res.raise_for_status()\n",
        "bsObj = bs4.BeautifulSoup(res.content , \"html.parser\")\n",
        "\n",
        "# テーブルを指定\n",
        "# table = bsObj.findAll(\"table\",{\"class\":\"tablesorter\"})[0]\n",
        "# タグとクラスが該当するテーブルを tables に入れておく。\n",
        "# これは、WEBページの記述に依存するので都度調べる必要がある。\n",
        "# Sdescription = \"table\"\n",
        "# Ddescription = {\"class\" : \"style71\"}\n",
        "tables = bsObj.findAll(Sdescription, Ddescription )\n",
        "\n",
        "csvFileName ='chiiki_work.csv'\n",
        "\n",
        "csvFile = open(csvFileName , 'wt', newline = '', encoding = 'utf-8')\n",
        "writer = csv.writer(csvFile)\n",
        "\n",
        "try:\n",
        "  # 地域名（『と地域』とか）をキーとして国名のリストを値とした辞書\n",
        "  dictChiiki = {}\n",
        "  # 国名（『英国』とか）をキーとして属している地域のリスト（['と地域']とか）を値とした辞書\n",
        "  dictCountry = {}\n",
        "\n",
        "  # tables の要素としてひとつひとつの table ごとに\n",
        "  for table in tables:\n",
        "\n",
        "    # タグ《tr》により行（rows）を検出して\n",
        "    rows = table.findAll(\"tr\")\n",
        "    \n",
        "    '''\n",
        "# 行の処理（１行おきに定義地域名だけのリストと当該地域のリスト）を\n",
        "# 制御するための制御変数を｛■初期化：□更新｝\n",
        "    '''\n",
        "    coline = 0\n",
        "    listRow1 =[]    \n",
        "    \n",
        "    # rows の要素としてのひとつひとつの row ごとに\n",
        "    for row in rows:\n",
        "      \n",
        "      '''\n",
        "# 行の処理（１行おきに定義地域名だけのリストと当該地域のリスト）を\n",
        "# 制御するための制御変数を｛□初期化：■更新｝\n",
        "      '''\n",
        "      coline = coline + 1      \n",
        "            \n",
        "      # セルを csvRow にリストアップしていく\n",
        "      # （実際には今回の対象の表では行あたりひとつのセルしかないが）\n",
        "      csvRow = []\n",
        "      for cell in row.findAll(['td', 'th']):\n",
        "        # もともとのページの作りが、文字列の前後がわりと雑に\n",
        "        # 空白などが入っていて邪魔なので、それらをここで切りおとす。\n",
        "        csvRow.append(cell.get_text().lstrip().rstrip())\n",
        "        # 行あたりのセルがひとつなので、結局、リストの最初の要素である\n",
        "        # csvRow[0] （中身は表のセルに記載されていた文字列）だけを処理\n",
        "        # すればよいのだが、それが地域の定義名（『と地域』とか）の場合\n",
        "        # も、その地域に含まれる国名などを『、』で区切った文字列である\n",
        "        # 場合もある。後者の場合だけ『、』が含まれているので、結局、\n",
        "        # 毎回、『,』で区切って split() でリストにしておく。リストの\n",
        "        # 要素が国名等であるがその前後の空白などは閉包で要素に対して\n",
        "        # strip しておく。\n",
        "        listRow0 = listRow1\n",
        "        listRow1 = [ x.strip() for x in csvRow[0].split(\"、\") ]\n",
        "        # print(coline)\n",
        "        # pprint.pprint(listRow1)\n",
        "        \n",
        "        # ２行分そろったら\n",
        "        if (coline%2==0) :\n",
        "          # リストとしてどうなっているかちょっと見てみる。\n",
        "          # pprint.pprint(listRow0)\n",
        "          # pprint.pprint(listRow1)\n",
        "          \n",
        "          # 辞書に登録\n",
        "          # dictChiiki[listRow0[0]] = len(listRow1)\n",
        "          dictChiiki[listRow0[0]] = listRow1\n",
        "          \n",
        "          # 逆引き辞書にも登録\n",
        "          Schiiki = listRow0[0]\n",
        "          for Scountry in listRow1 :\n",
        "            if (Scountry in dictCountry.keys()):\n",
        "              Lchiiki = dictCountry[Scountry]\n",
        "            else:\n",
        "              Lchiiki = []\n",
        "            Lchiiki.append(Schiiki)\n",
        "            dictCountry[Scountry] = Lchiiki\n",
        "        \n",
        "        # writer.writerow(csvRow)\n",
        "        # writer.writerow(pprint.pformat(listRow))\n",
        "#\n",
        "finally:\n",
        "    csvFile.close()\n",
        "#\n",
        "with open(csvFileName , 'rt') as file:\n",
        "  s = file.read()\n",
        "  print(s)\n",
        "#\n",
        "#\n",
        "# pprint.pprint(dictChiiki)\n",
        "# pprint.pprint(dictCountry)\n",
        "print( hashlib.md5(pprint.pformat(dictCountry).encode()).hexdigest() )\n",
        "# b1d74ca20ed9e0efed3ab6c5f21c991d\n",
        "# pprint.pprint(dictCountry)\n",
        "# Scountry = 'アメリカ合衆国'\n",
        "# Scountry = '北朝鮮'\n",
        "# Scountry = '中国'\n",
        "# Scountry = '中華人民共和国'\n",
        "Scountry = ''\n",
        "\n",
        "if len(Scountry)==0 :\n",
        "  print('====国名等は以下のどれかでしょうか====')\n",
        "  for k in dictCountry.keys() :\n",
        "    print(k)\n",
        "  print('====国名等の逆引き辞書は以下の通り====')\n",
        "  pprint.pprint(dictCountry)\n",
        "else:\n",
        "  print(Scountry)\n",
        "  if ( Scountry in dictCountry.keys() ):\n",
        "    pprint.pprint(dictCountry[Scountry])\n",
        "  else:\n",
        "    print('⇒（該当国名がありません）その他の地域')\n",
        "    pprint.pprint(dictCountry['その他の地域'])\n",
        "    # print(dictCountry['その他の地域'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WTIywrs52eGh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7abfdc62-34aa-451f-ba76-03942ed75fdc"
      },
      "cell_type": "code",
      "source": [
        "### -*- coding:utf-8 -*-\n",
        "# METI_chiiki.py\n",
        "\n",
        "# 対象とする Web ページから決める記述は以下の３行。以下は\n",
        "# 経産省の国と地域のページから地域の定義表を取り出すため\n",
        "# に必要となる情報\n",
        "Surl = \"http://www.meti.go.jp/policy/anpo/apply08.html\"\n",
        "Sdescription = \"table\"\n",
        "Ddescription = {\"class\" : \"style71\"}\n",
        "\n",
        "# https://qiita.com/hujuu/items/b0339404b8b0460087f9\n",
        "# これを参考にさせていただきました。ありがとうございます。\n",
        "\n",
        "import pprint\n",
        "import csv\n",
        "import requests\n",
        "import bs4\n",
        "import hashlib\n",
        "\n",
        "def print_chiiki(s_country):\n",
        "  # URLを指定してコンテンツを取り出す\n",
        "  #\n",
        "  '''\n",
        "html = urlopen(\"http://www.meti.go.jp/policy/anpo/apply08.html\")\n",
        "bsObj = BeautifulSoup(html, \"html.parser\")\n",
        "# urlopenだとうまく動かないので、requests.get で .content を得るようにしました。\n",
        "# これは\n",
        "# https://orangain.hatenablog.com/entry/encoding-in-requests-and-beautiful-soup\n",
        "# これを参考にさせていただきました。ありがとうございます。\n",
        "# chardet も入れておいたほうがいいのかもしれませんが、今は入れてません。\n",
        "  '''\n",
        "  # 文法解析されたコンテンツを bsObj に入れておく\n",
        "  # Surl = \"http://www.meti.go.jp/policy/anpo/apply08.html\"\n",
        "  res = requests.get(Surl)\n",
        "  res.raise_for_status()\n",
        "  bsObj = bs4.BeautifulSoup(res.content , \"html.parser\")\n",
        "\n",
        "  # テーブルを指定\n",
        "  # table = bsObj.findAll(\"table\",{\"class\":\"tablesorter\"})[0]\n",
        "  # タグとクラスが該当するテーブルを tables に入れておく。\n",
        "  # これは、WEBページの記述に依存するので都度調べる必要がある。\n",
        "  # Sdescription = \"table\"\n",
        "  # Ddescription = {\"class\" : \"style71\"}\n",
        "  tables = bsObj.findAll(Sdescription, Ddescription )\n",
        "\n",
        "  # csvFileName ='chiiki_work.csv'\n",
        "  # csvFile = open(csvFileName , 'wt', newline = '', encoding = 'utf-8')\n",
        "  # writer = csv.writer(csvFile)\n",
        "\n",
        "  try:\n",
        "    # 地域名（『と地域』とか）をキーとして国名のリストを値とした辞書\n",
        "    dictChiiki = {}\n",
        "    # 国名（『英国』とか）をキーとして属している地域のリスト（['と地域']とか）を値とした辞書\n",
        "    dictCountry = {}\n",
        "\n",
        "    # tables の要素としてひとつひとつの table ごとに\n",
        "    for table in tables:\n",
        "\n",
        "      # タグ《tr》により行（rows）を検出して\n",
        "      rows = table.findAll(\"tr\")\n",
        "    \n",
        "      '''\n",
        "# 行の処理（１行おきに定義地域名だけのリストと当該地域のリスト）を\n",
        "# 制御するための制御変数を｛■初期化：□更新｝\n",
        "      '''\n",
        "      coline = 0\n",
        "      listRow1 =[]    \n",
        "    \n",
        "      # rows の要素としてのひとつひとつの row ごとに\n",
        "      for row in rows:\n",
        "      \n",
        "        '''\n",
        "# 行の処理（１行おきに定義地域名だけのリストと当該地域のリスト）を\n",
        "# 制御するための制御変数を｛□初期化：■更新｝\n",
        "        '''\n",
        "        coline = coline + 1      \n",
        "            \n",
        "        # セルを csvRow にリストアップしていく\n",
        "        # （実際には今回の対象の表では行あたりひとつのセルしかないが）\n",
        "        csvRow = []\n",
        "        for cell in row.findAll(['td', 'th']):\n",
        "          # もともとのページの作りが、文字列の前後がわりと雑に\n",
        "          # 空白などが入っていて邪魔なので、それらをここで切りおとす。\n",
        "          csvRow.append(cell.get_text().lstrip().rstrip())\n",
        "          # 行あたりのセルがひとつなので、結局、リストの最初の要素である\n",
        "          # csvRow[0] （中身は表のセルに記載されていた文字列）だけを処理\n",
        "          # すればよいのだが、それが地域の定義名（『と地域』とか）の場合\n",
        "          # も、その地域に含まれる国名などを『、』で区切った文字列である\n",
        "          # 場合もある。後者の場合だけ『、』が含まれているので、結局、\n",
        "          # 毎回、『,』で区切って split() でリストにしておく。リストの\n",
        "          # 要素が国名等であるがその前後の空白などは閉包で要素に対して\n",
        "          # strip しておく。\n",
        "          listRow0 = listRow1\n",
        "          listRow1 = [ x.strip() for x in csvRow[0].split(\"、\") ]\n",
        "          # print(coline)\n",
        "          # pprint.pprint(listRow1)\n",
        "        \n",
        "          # ２行分そろったら\n",
        "          if (coline%2==0) :\n",
        "            # リストとしてどうなっているかちょっと見てみる。\n",
        "            # pprint.pprint(listRow0)\n",
        "            # pprint.pprint(listRow1)\n",
        "          \n",
        "            # 辞書に登録\n",
        "            # dictChiiki[listRow0[0]] = len(listRow1)\n",
        "            dictChiiki[listRow0[0]] = listRow1\n",
        "          \n",
        "            # 逆引き辞書にも登録\n",
        "            Schiiki = listRow0[0]\n",
        "            for Scountry in listRow1 :\n",
        "              if (Scountry in dictCountry.keys()):\n",
        "                Lchiiki = dictCountry[Scountry]\n",
        "              else:\n",
        "                Lchiiki = []\n",
        "              Lchiiki.append(Schiiki)\n",
        "              dictCountry[Scountry] = Lchiiki\n",
        "        \n",
        "          # writer.writerow(csvRow)\n",
        "          # writer.writerow(pprint.pformat(listRow))\n",
        "  #\n",
        "  finally:\n",
        "    # csvFile.close()\n",
        "    print('done')\n",
        "  #\n",
        "  s_ret = \"\"\n",
        "  if len(s_country)==0 :\n",
        "    s_ret = '====国名等は以下のどれかでしょうか====\\n'\n",
        "    for k in dictCountry.keys() :\n",
        "      s_ret = s_ret + k + '\\n'\n",
        "    s_ret = s_ret + '====国名等の逆引き辞書は以下の通り====\\n'\n",
        "    s_ret = s_ret + pprint.pformat(dictCountry)\n",
        "  else:\n",
        "    s_ret = s_country + ' '\n",
        "    if ( s_country in dictCountry.keys() ):\n",
        "      s_ret = s_ret + pprint.pformat(dictCountry[Scountry])\n",
        "    else:\n",
        "      s_ret = s_ret + '⇒（該当国名がありません）その他の地域\\n'\n",
        "      s_ret = s_ret + pprint.pprint(dictCountry['その他の地域'])\n",
        "    #\n",
        "    # print(dictCountry['その他の地域'])\n",
        "  #\n",
        "  return s_ret\n",
        "#\n",
        "'''\n",
        "with open(csvFileName , 'rt') as file:\n",
        "  s = file.read()\n",
        "  print(s)\n",
        "'''\n",
        "#\n",
        "#\n",
        "# pprint.pprint(dictChiiki)\n",
        "# pprint.pprint(dictCountry)\n",
        "#print( hashlib.md5(pprint.pformat(dictCountry).encode()).hexdigest() )\n",
        "# b1d74ca20ed9e0efed3ab6c5f21c991d\n",
        "# pprint.pprint(dictCountry)\n",
        "# Scountry = 'アメリカ合衆国'\n",
        "# Scountry = '北朝鮮'\n",
        "# Scountry = '中国'\n",
        "# Scountry = '中華人民共和国'\n",
        "Scountry = ''\n",
        "'''\n",
        "if len(Scountry)==0 :\n",
        "  print('====国名等は以下のどれかでしょうか====')\n",
        "  for k in dictCountry.keys() :\n",
        "    print(k)\n",
        "  print('====国名等の逆引き辞書は以下の通り====')\n",
        "  pprint.pprint(dictCountry)\n",
        "else:\n",
        "  print(Scountry)\n",
        "  if ( Scountry in dictCountry.keys() ):\n",
        "    pprint.pprint(dictCountry[Scountry])\n",
        "  else:\n",
        "    print('⇒（該当国名がありません）その他の地域')\n",
        "    pprint.pprint(dictCountry['その他の地域'])\n",
        "    # print(dictCountry['その他の地域'])\n",
        "'''\n",
        "\n",
        "print_chiiki('アメリカ合衆国')\n",
        "\n",
        "# https://us-central1-aerobic-factor-216914.cloudfunctions.net/function-4"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"アメリカ合衆国 ['ろ地域', 'は地域②', 'に地域①', 'へ地域', 'ち地域']\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    }
  ]
}